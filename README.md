<div align="center">

# 🧠 Contextinator

**Intelligent Codebase Understanding for AI Agents**

_Transform any codebase into semantically-aware, searchable knowledge for AI-powered workflows_

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![ChromaDB](https://img.shields.io/badge/vectorstore-ChromaDB-orange.svg)](https://www.trychroma.com/)
[![openAI](https://img.shields.io/badge/embeddings-OpenAI-blue.svg)](https://www.trychroma.com/)
</div>

---

## 📖 Overview

**Contextinator** is a powerful tool that bridges the gap between static codebases and intelligent AI agents. It uses Abstract Syntax Tree (AST) parsing to extract semantic code chunks, generates embeddings, and stores them in a vector database-enabling AI agents to understand, navigate, and reason about codebases with unprecedented precision.

### ✨ Key Features

- 🌳 **AST-Powered Chunking** - Extract functions, classes, and methods from 23+ programming languages
- 🔍 **Semantic Search** - Find relevant code using natural language queries
- 🚀 **Full Pipeline Automation** - One command to chunk, embed, and store
- 🎯 **Smart Deduplication** - Hash-based detection of duplicate code
- 📊 **Visual AST Explorer** - Debug and visualize code structure
- 🌐 **Web Viewer** - Browse your embeddings with a clean UI
- 🐳 **Docker-Ready** - ChromaDB server included

### 🎯 Use Cases

- **AI Code Assistants** - Give LLMs deep codebase understanding
- **Documentation Generation** - Auto-generate docs from code structure
- **Code Search & Discovery** - Find implementations across large projects
- **Refactoring Analysis** - Identify duplicate or similar code patterns
- **Onboarding Automation** - Help new developers navigate unfamiliar codebases

---

## ⚠️ Warning
all the content below this line is generated by AI and may contain inaccuracies. Project is still under active development and things may change rapidly. Please refer to the source code for the most up-to-date information.


## 🚀 Quick Start

### Prerequisites

- Python 3.11 or higher
- Docker (for ChromaDB)
- OpenAI API key (for embeddings)

### Installation

**Step 1:** Clone and setup

```bash
git clone https://github.com/starthackHQ/Contextinator.git
cd Contextinator
```

**Step 2:** Create virtual environment

```bash
python -m venv .venv
```

**Step 3:** Activate environment

```bash
# Windows
.venv\Scripts\activate

# macOS/Linux
source .venv/bin/activate
```

**Step 4:** Install dependencies

```bash
pip install -r requirements.txt
```

**Step 5:** Configure environment

Create a `.env` file:

```bash
OPENAI_API_KEY=your_openai_api_key_here
USE_CHROMA_SERVER=true
CHROMA_SERVER_URL=http://localhost:8000
```

**Step 6:** Start ChromaDB

```bash
docker-compose up -d
```

### 🎬 First Run

Process your codebase in one command:

```bash
# Chunk, embed, and store the current directory
python -m src.cli chunk-embed-store-embeddings --save

# Or process a remote repository
python -m src.cli chunk-embed-store-embeddings --save --repo-url https://github.com/username/repo
```

**That's it!** Your codebase is now indexed and ready for semantic search.

---

## 💡 Usage Guide

### 1. Chunking Code

Extract semantic code chunks using AST parsing:

**Basic chunking:**

```bash
# Chunk current directory → saves to .chunks/chunks.json
python -m src.cli chunk --save
```

**With AST visualization (recommended for debugging):**

```bash
python -m src.cli chunk --save --save-ast
```

**Chunk specific directory:**

```bash
python -m src.cli chunk --save --path /path/to/your/repo
```

**Custom output location:**

```bash
python -m src.cli chunk --save --output /custom/output/dir
```

### 2. Chunk Remote Repository

Clone and process GitHub repositories directly:

```bash
# Basic remote chunking
python -m src.cli chunk --save --repo-url https://github.com/username/repo

# With AST visualization
python -m src.cli chunk --save --save-ast --repo-url https://github.com/username/repo
```

### 3. Understanding the Output

After chunking, you'll find:

```
.chunks/
├── chunks.json          # 📦 All extracted code chunks with metadata
└── ast_trees/          # 🌳 AST visualizations (if --save-ast used)
    ├── file1_python_ast.json
    ├── file2_javascript_ast.json
    └── ast_overview.json
```

**Example `chunks.json` structure:**

```json
{
  "chunks": [
    {
      "type": "function_definition",
      "name": "my_function",
      "content": "def my_function():\n    return 'hello'",
      "file_path": "/path/to/file.py",
      "language": "python",
      "start_line": 10,
      "end_line": 12,
      "hash": "abc123..."
    }
  ],
  "statistics": {
    "unique_hashes": 150,
    "duplicates_found": 5
  }
}
```

---

## 🌍 Supported Languages

**23 Languages with Full AST Support:**

| Category          | Languages                                               |
| ----------------- | ------------------------------------------------------- |
| **Backend**       | Python • Java • Go • Rust • C • C++ • C# • PHP • Kotlin |
| **Frontend**      | JavaScript • TypeScript • TSX                           |
| **Scripting**     | Bash • Lua                                              |
| **Database**      | SQL                                                     |
| **Config**        | YAML • JSON • TOML • Dockerfile                         |
| **Documentation** | Markdown                                                |
| **Blockchain**    | Solidity                                                |
| **Mobile**        | Swift • Kotlin                                          |

### What Gets Extracted?

Each language extracts semantic units intelligently:

| Element               | Examples                                                   |
| --------------------- | ---------------------------------------------------------- |
| **Functions/Methods** | Function definitions, arrow functions, method declarations |
| **Classes**           | Class declarations, interfaces, structs                    |
| **Other**             | Properties, objects, tables, commands (language-specific)  |

---

## 📚 CLI Reference

### Core Commands

#### 🔹 `chunk --save`

**Extract semantic code chunks using Tree-sitter AST parsing.**

| Option           | Description                                     |
| ---------------- | ----------------------------------------------- |
| `--save`         | Save chunks to `.chunks/chunks.json`            |
| `--save-ast`     | Generate AST visualizations for debugging       |
| `--path PATH`    | Path to repository (default: current directory) |
| `--output DIR`   | Output directory (default: current directory)   |
| `--repo-url URL` | Clone and chunk remote repository               |

**Examples:**

```bash
python -m src.cli chunk --save
python -m src.cli chunk --save --save-ast
python -m src.cli chunk --save --repo-url https://github.com/user/repo
```

---

#### 🔹 `embed --save`

**Generate embeddings using OpenAI's `text-embedding-3-large` model.**

| Option           | Description                                      |
| ---------------- | ------------------------------------------------ |
| `--save`         | Save embeddings to `.embeddings/embeddings.json` |
| `--path PATH`    | Path to repository (default: current directory)  |
| `--repo-url URL` | Clone and embed remote repository                |

**Requirements:**

- ✅ Set `OPENAI_API_KEY` in `.env` file
- ✅ Chunks must exist (run `chunk --save` first)

**Examples:**

```bash
python -m src.cli embed --save
python -m src.cli embed --save --path /path/to/repo
```

---

#### 🔹 `store-embeddings`

**Load embeddings into ChromaDB vector store.**

| Option                   | Description                                       |
| ------------------------ | ------------------------------------------------- |
| `--path PATH`            | Path to repository (default: current directory)   |
| `--repo-url URL`         | Remote repository URL                             |
| `--collection-name NAME` | Custom collection name (default: repository name) |

**Requirements:**

- ✅ ChromaDB server running: `docker-compose up -d`
- ✅ Embeddings must exist (run `embed --save` first)

**Examples:**

```bash
python -m src.cli store-embeddings --path .
python -m src.cli store-embeddings --path . --collection-name MyProject
```

---

#### 🔹 `chunk-embed-store-embeddings`

**🚀 Full pipeline: chunk → embed → store in one command.**

| Option                   | Description                                       |
| ------------------------ | ------------------------------------------------- |
| `--save`                 | Save intermediate artifacts (chunks + embeddings) |
| `--path PATH`            | Path to repository (default: current directory)   |
| `--repo-url URL`         | Clone and process remote repository               |
| `--collection-name NAME` | Custom collection name                            |

**Examples:**

```bash
# Process current directory
python -m src.cli chunk-embed-store-embeddings --save

# Process remote repository
python -m src.cli chunk-embed-store-embeddings --save --repo-url https://github.com/user/repo

# Custom collection name
python -m src.cli chunk-embed-store-embeddings --save --collection-name MyProject
```

---

### Database Commands

#### 🔹 `db-info`

**Display ChromaDB database statistics.**

```bash
python -m src.cli db-info
```

**Output:**

- 📊 List of all collections
- 📈 Document counts per collection
- 🔢 Total documents across all collections

---

#### 🔹 `db-list`

**List all collections in ChromaDB.**

```bash
python -m src.cli db-list
```

---

#### 🔹 `db-show`

**Show details of a specific collection.**

| Argument          | Description                       |
| ----------------- | --------------------------------- |
| `collection_name` | Name of the collection (required) |
| `--sample N`      | Show N sample documents           |

**Examples:**

```bash
python -m src.cli db-show Contextinator
python -m src.cli db-show Contextinator --sample 5
```

---

#### 🔹 `db-clear`

**Delete a specific collection from ChromaDB.**

| Argument          | Description                                 |
| ----------------- | ------------------------------------------- |
| `collection_name` | Name of the collection to delete (required) |
| `--force`         | Skip confirmation prompt                    |

**Examples:**

```bash
python -m src.cli db-clear Contextinator
python -m src.cli db-clear Contextinator --force
```

---

#### 🔹 `query`

**⚠️ Coming Soon** - Semantic search on vector store.

```bash
python -m src.cli query "implement user authentication in Python"
python -m src.cli query "myquery" --save results.txt --n-results 10
```

---

## 🗄️ ChromaDB Setup

### Start the Server

```bash
# Start ChromaDB server (runs on port 8000)
docker-compose up -d

# Check if running
docker ps | grep chroma

# Stop server
docker-compose down
```

---

## 🖥️ Web Viewer

### Launch the Viewer

Browse your code embeddings with a clean web interface:

```bash
python viewer.py
```

Then open **http://localhost:5001** in your browser.

### Features

✅ Browse all collections  
✅ View chunks with metadata (file path, type, line numbers)  
✅ Paginated navigation  
✅ Clean, responsive interface

---

## ⚙️ Configuration

### Environment Variables

Create a `.env` file in the project root:

```bash
# OpenAI API Key (required for embeddings)
OPENAI_API_KEY=your_openai_api_key_here

# ChromaDB Configuration
USE_CHROMA_SERVER=true
CHROMA_SERVER_URL=http://localhost:8000
```

### Chunking Settings

Edit `src/config/settings.py` to customize:

```python
MAX_TOKENS = 512              # Maximum tokens per chunk
CHUNK_OVERLAP = 50            # Overlap between chunks (for context continuity)
DEFAULT_EMBEDDING_MODEL = 'text-embedding-3-large'  # OpenAI model
```

---

## 🔧 Advanced Features

### AST Visualization

Debug and understand how your code is parsed:

```bash
python -m src.cli chunk --save --save-ast
```

**Output structure:**

```
.chunks/ast_trees/
├── MyFile_python_ast.json     # 🌳 Individual file AST
├── AnotherFile_java_ast.json
└── ast_overview.json          # 📋 Summary of all files
```

**Each AST file contains:**

- 🌲 Complete AST tree structure
- 🎯 Extracted semantic nodes (functions, classes)
- 📊 Tree statistics (depth, node count)
- 🗺️ Source code mappings (line numbers, byte positions)

### Ignore Patterns

The tool automatically ignores common build artifacts and dependencies:

| Language/Framework  | Ignored Patterns                                 |
| ------------------- | ------------------------------------------------ |
| **Python**          | `__pycache__`, `.venv`, `*.pyc`, `.pytest_cache` |
| **JavaScript/Node** | `node_modules`, `*.min.js`, `.next`, lock files  |
| **Java/Kotlin**     | `target`, `*.class`, `.gradle`                   |
| **C/C++**           | `*.o`, `*.so`, `CMakeFiles`                      |
| **Rust**            | `target`, `Cargo.lock`                           |
| **Go**              | `vendor`                                         |
| **C#/.NET**         | `bin`, `obj`, `.vs`                              |
| **PHP**             | `vendor`, `composer.lock`                        |
| **Swift**           | `.build`, `DerivedData`                          |
| **General**         | `.git`, `.idea`, `.vscode`, `*.log`              |

📝 **See `src/config/settings.py` for the complete list**

---

## 🛠️ Installation

### Quick Install

```bash
pip install -r requirements.txt
```

### Manual Installation

If you prefer to install packages individually:

```bash
pip install tree-sitter chromadb \
  tree-sitter-python tree-sitter-javascript tree-sitter-typescript \
  tree-sitter-java tree-sitter-go tree-sitter-rust \
  tree-sitter-cpp tree-sitter-c tree-sitter-c-sharp \
  tree-sitter-php tree-sitter-bash tree-sitter-sql \
  tree-sitter-kotlin tree-sitter-yaml tree-sitter-markdown \
  tree-sitter-dockerfile tree-sitter-json tree-sitter-toml \
  tree-sitter-swift tree-sitter-solidity tree-sitter-lua
```

---

## 🐛 Troubleshooting

### No chunks generated?

**Possible causes:**

✅ Check if your files have supported extensions  
✅ Verify files aren't in ignore patterns  
✅ Use `--save-ast` to debug AST parsing  
✅ Ensure you're in a directory with code files

**Debug command:**

```bash
python -m src.cli chunk --save --save-ast
# Check .chunks/ast_trees/ for parsing details
```

---

### Tree-sitter import errors?

**Solution:**

Install missing language modules:

```bash
pip install tree-sitter-<language>
```

**Note:** The tool will fallback to file-level chunking if parsers are missing.

---

### Empty chunks.json?

**Check list:**

✅ Ensure you're in a directory with code files  
✅ Verify file extensions match `SUPPORTED_EXTENSIONS` in `settings.py`  
✅ Check if files are being excluded by ignore patterns

---

### ChromaDB connection errors?

**Solution:**

```bash
# Verify ChromaDB is running
docker ps | grep chroma

# If not running, start it
docker-compose up -d

# Check logs if issues persist
docker logs <container_id>
```

---

### OpenAI API errors?

**Check:**

✅ `OPENAI_API_KEY` is set in `.env`  
✅ API key is valid and has credits  
✅ You're using the correct embedding model

---

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## 🤝 Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## 📞 Support

If you encounter any issues or have questions, please [open an issue](https://github.com/starthackHQ/Contextinator/issues) on GitHub.

---

<div align="center">

**Made with ❤️ for AI Agents**

⭐ Star this repo if you find it useful!

</div>
